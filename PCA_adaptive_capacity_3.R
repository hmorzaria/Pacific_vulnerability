# Hem Nalini Morzaria Luna
# Vulnerability indices
#based on Jacob et al. 2012 & Morzaria-Luna et al. 2014
#Created July 2015
#Last updated July 2015

#install.packages(c("data.table","gdata","tidyr","pipeR","erer","psych","nFactors","tidyr","pipeR","erer","reshape","ggplot2","dplyr","FactoMineR"))
library(nFactors)
library(reshape)
library(ggplot2)
library(dplyr)
library(gdata)
library(data.table)
library(tidyr)
library(pipeR)
library(erer)
library(FactoMineR)
library(magrittr)


rm(list=ls()) #clean up the space

#list cutom functions
#source("E:/Archivos/1Archivos/Articulos/R_functions/pause_function.R")
source("E:/Archivos/1Archivos/Articulos/R_functions/Theta_function.R")

# USER BLOCK: CHECK AND CHANGE OPTIONS HERE  
#_________________________________________________________________________
#set working directories
#this should match the path where your files directories are stored
#note the "/" go in the opposite direction than in Windows explorer

threshold.value = 0.5 #this is the threshold for correlation values

#working space
analysispath="E:/Archivos/1Archivos/Articulos/En preparacion/Vulnerability_GOC_Pacific/Analysis/ResultsINEGI" #'put path

setwd(analysispath)

indices.sc = c("PopulationComposition_AC")

#Analysis expects csv files with locations as 1st column and variables as other columns, values in rows
files.index = c("variables_population_comp.csv")


#_________________________________________________________________________

#make list to save results
results.index=list()
#use for debugging
each.index= 1
#for(each.index in 1:length(files.index))
#{
index.name = indices.sc[each.index]

#piped code
#To debug this code, insert (~ browser()) around the pipeline 
#At the browser environment, one only needs to type .
#works with %>>%
#obtain correlation matrix
#get correlation matrix and statistics
#X11()

print("Correlations from full set of data")
#pause()

index.data <- files.index[each.index] %>%
fread(header = T,drop=1) %>% 
  tbl_df %>% dplyr::select(-starts_with("POPCHANGE")) %>% #2559 final communities
      na.omit %>% 
  select(-P_3YMAS_F, -P_5YMAS_F, -P_12YMAS_F, -P_3YMAS_M, -P_5YMAS_M, -P_12YMAS_M, -P_15YMAS_F, -P_18YMAS_F, -P_15YMAS_M, -P_18YMAS_M)
   # index.data %>% dplyr::select(-POPCHANGE_90_10,-POPCHANGE_90_00,-POPCHANGE_00_10) %>% #eliminates all population change prior to 2000, leaves 2428 communities


corr.mat <- index.data  %>% 
  cor 

no.cols <- corr.mat %>% ncol

no.rows <- corr.mat %>% nrow

 parallel(subject=no.rows,var=no.cols,rep=100,cent=.05) %>% { #scree test
    .$eigen %>% 
     select(qevpea) %>% 
    nScree(eigen(cor(index.data))$values,(.$qevpea)) %>>% {
    	summary (.) #scree test will return number of factors according to different methods
    	plotnScree((.), main=paste("Non Graphical Solutions to Scree Test _",index.name)) 
    }
  }
  

# Determine Number of Factors to Extract
   #This code can be use to determine the optimal number of factors, however Jacob et al (2012)
  #recommends using a single factor solution
  # Generally, if the goal is simple and parsimonious description of a correlation 
  # or covariance matrix, the first k principal components 
  # will do a better job than any other k-dimensional solution
  
#save the scree plot as a png file
dev.copy(png, paste(index.name,'.png',sep="")) 
dev.off()

#now take correlation matrix and make data frame

index.data %>% 
	cor %>% 
  as.data.frame %>>%
  (~ corr.mat)%>%
  names %>% 
  as.data.frame %>>%
  (~ names.mat) 
  
names(names.mat) = ("corr_variable")

print("Now removing variables with low correlation values")
#pause()

cbind(names.mat,corr.mat) %>% 
  gather(variable,value,-corr_variable) %>% 
filter(value!=1) %>%
filter(value!=-1) %>%
  group_by(variable) %>%
  summarize(min_corr = abs(min(value))) %>% 
  filter(min_corr > threshold.value) %>>% 
  select(variable) %>>%
  (~selected.vars)

print("Iteratively change the threshold value and then run the scree plot again")

if(nrow(selected.vars) < 4)
  {
    print("Analysis needs at least 4 variables, now running with lower correlation threshold value")
  threshold.value = threshold.value - 0.05
  cbind(names.mat,corr.mat) %>% 
    gather(variable,value,-corr_variable) %>% 
    filter(value!=1) %>%
    filter(value!=-1) %>%
    group_by(variable) %>%
    summarize(mean_corr = abs(mean(value))) %>% 
    filter(mean_corr > threshold.value) %>>% 
    select(variable) %>>%
    (~selected.vars)

  if(nrow(selected.vars) < 4)
     {
     print("Too few variables selected, now running with lower correlation threshold value")
      threshold.value = threshold.value - 0.05
    cbind(names.mat,corr.mat) %>% 
      gather(variable,value,-corr_variable) %>% 
      filter(value!=1) %>%
      filter(value!=-1) %>%
      group_by(variable) %>%
      summarize(mean_corr = abs(mean(value))) %>% 
      filter(mean_corr > threshold.value) %>>% 
      select(variable) %>>%
      (~selected.vars)
    }
    } else {
#       
    selected.vars %>>% 
      (variable) %>%
      drop.levels %>>%
      (~good.variables = as.character(.))

#     
#     #X11()
#     
print("Correlations from subset of data with largest correlations")

#     #pause()
#     
    select(index.data,one_of(good.variables))  %>>% 
      (~subset.index.data = as.data.frame(.)) %>% #subset the original dataframe leaving only variables with highest correlations
      cor %>>% #again calculate correlations and scree test
      (~ corr.mat) %>>% 
      (~ pairs(.)) 
    
    
    corr.mat %>>% 
      (~no.cols <- ncol(.)) %>>% 
      (~no.rows <- nrow(.))
      
    
        parallel(subject=no.rows,var=no.cols,rep=100,cent=.05) %>>% #scree test
          (eigen) %>% 
          .$qevpea %>>% (~qepva)
        
          nScree(eigen(cor(subset.index.data))$values,qepva) %>>% 
          ~ plotnScree((.), main=paste("Non Graphical Solutions to Scree Test _",index.name)) 
            
       
      print("The result of the scree test should be one")
    #pause()
    
    #once extra variables have been eliminated
    #run single factor solution
    #can specify rotate="varimax" in pc
    #SS loading is the eigenvalue
    # h2is called the communality estimate. Measures the % of variance 
    # in an observed variable accounted for by the retained components
    # factor.model finds the reproduced correlations and the communalities (the diagonals)
    #Calculate theta coefficient
    
    files.index[each.index] %>%
      fread(header = T) %>% 
      .$V1 %>>% 
      (~rownames(subset.index.data) <- (.)) %>>% 
      (~names.locs = as.data.frame(.))
    
    subset.index.data %>% 
        PCA %>>% 
      (~pca.results)  %>>% 
      (eig) %>% 
      select(eigenvalue) %>%
      max %>% Theta %>>%
      (~ theta.result)
    
  }


as.data.frame(theta.result) %>>% (~theta.result) 
  names(theta.result) <- "Armor's Theta"
  results.index$SelectedVariables =  selected.vars
  results.index$PCA = "$eig = eigenvalues and variance, $ind$coord = scores, $var$coord = loadings"
  results.index$eigenvalues.variance =  pca.results$eig
  results.index$loadings =  pca.results$var$coord
  results.index$scores =  pca.results$ind$coord
  results.index$Theta =  theta.result
  results.index$names_scores =  row.names(pca.results$ind$coord)

  #save site scores as a separate file
  names(names.locs) = "sites"
  pca.results$ind$coord %>% 
    cbind(names.locs) %>>%
    (~scores.pca)
 
  write.csv(scores.pca,paste(index.name,"_pca_scores.csv",sep=""))

  write.list(results.index,paste(index.name,"_pca_results.csv",sep=""))
# 

 adaptive_capacity = as.data.frame(matrix(0,nrow=nrow(scores.pca),ncol=0))
 NormFunc <- function(x) (x-min.score)/(max.score-min.score)

pca.files <- list.files(pattern = "\\AC_pca_scores.csv$")

for(each.file in 1:length(pca.files)){
  
  this.file = pca.files[each.file] # read each file
  
  this.index = unlist(strsplit((this.file),"_"))[1] #get index name
  
  pca.files[each.file] %>%  
    read.csv(header=T) %>% #read file
    select(Dim.1) %>>% #select sites and scores
    (~this.index.score)%>>% #calculate max and min of scores
      (~max.score <- max (.)) %>>%
      (~min.score <- min (.)) 
  
        NormFunc(this.index.score[,1]) %>% 
      as.data.frame %>>% 
      (~normalized)
    names(normalized) <- c(this.index)
    
     adaptive_capacity = cbind(adaptive_capacity, normalized)
      
}

 adaptive_capacity %>% 
  mutate(adaptive_capacity=rowSums(.)) %>>% 
   (~adaptive_capacity) 
 
 as.data.frame(NormFunc(adaptive_capacity[,"adaptive_capacity"])) %>>%
   (~normalized_AC)
 names(normalized_AC) <- c("normalized_AC")

pca.files[each.file] %>%  
   read.csv(header=T) %>% 
  select(sites) %>>% 
  cbind(normalized_AC)
   
write.csv(scores.pca,"AdaptiveCapacity_vul_scores.csv")
 